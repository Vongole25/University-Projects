# University-Projects
고려대학교 세종캠퍼스에서 수행한 프로젝트들 (2020 ~ 2026)

# 1. 서울시 공시지가 예측 모델

##  프로젝트 개요
- **과목**: 다차원자료분석PBL (3학년 1학기, 고려대학교 세종캠퍼스)  
- **기간**: 2023년 4월 – 2023년 6월  
- **목표**: 서울시 공시지가 데이터를 활용하여 예측 가능한 모델을 구축하고, 주요 영향 요인을 도출  

---

##  배경 및 목적
공시지가는 부동산 과세, 금융, 정책 결정에 중요한 지표이다.  
특히 서울시의 공시지가는 변동성이 크고, 산정 기준이 명확하지 않다는 한계가 있다.  
본 프로젝트에서는 서울시 공시지가 데이터를 분석하여 **머신러닝 기반 예측 모델을 구축**하고, 공시지가에 영향을 미치는 주요 요인을 탐색하고자 하였다.  

---

##  데이터 출처
- 통계지리정보서비스 (SGIS)  
- 주민등록인구통계  
- 국토정보플랫폼  
- 국가공간정보포털  
- 건축데이터 민간개방시스템  
- 주소기반산업지원서비스  
- 공공데이터포털  

---

##  방법론

1. **파생변수 설정 (Feature Engineering)**  
   - 비율 변수, 범주형 인코딩, 상호작용 변수 등을 생성  
   - 원천 데이터를 활용해 설명력을 높이는 새로운 변수를 도출  

2. **탐색적 데이터 분석 (EDA)**  
   - 결측치 처리 및 이상치 탐지  
   - 변수 분포 및 상관관계 시각화  
   - 다중공선성 및 중복 변수 파악  

3. **인자분석 (Factor Analysis)**  
   - 상관된 변수들에서 잠재 요인 추출  
   - 해석 가능성을 유지하면서 차원 축소  

4. **변수 선택 (Feature Selection)**  
   - 상관분석 및 다중공선성 진단을 통해 주요 변수 선별  
   - 공시지가 예측에 가장 관련성이 높은 변수만 활용  

5. **차원 축소 (PCA)**  
   - 주성분분석을 적용하여 고차원 데이터를 요약  
   - 전체 분산의 주요 부분을 설명하는 성분만 보존  

6. **군집 분석 (Clustering)**  
   - PCA 변환 데이터를 활용하여 유사한 지역을 그룹화  
   - 지역별 특성을 반영한 패턴 도출  

7. **예측 모델링 (Predictive Modeling)**  
   - 다중 회귀, 랜덤포레스트, XGBoost 모델 구축 및 비교  
   - R², RMSE 지표로 성능 평가  

---

##  결과 및 결론

- **군집별 모델링을 적용했을 때 성능이 크게 향상됨**  
  - 군집화를 하지 않고 전체 데이터를 XGBoost로 학습했을 때 R² = **58.78%**  
  - 군집화 후 모든 클러스터에서 성능이 개선됨  

- **군집별 모델 성능**  
  | 클러스터   | 최적 모델       | R² 점수 |
  |------------|----------------|---------|
  | Cluster_0  | XGBoost        | **86.00%** |
  | Cluster_1  | XGBoost        | 70.95%   |
  | Cluster_2  | XGBoost        | 81.17%   |
  | Cluster_3  | Random Forest  | 81.32%   |

- **주요 인사이트**  
  - **Cluster_3**(생산인구가 많은 상업지역)을 제외한 모든 클러스터에서 **XGBoost**가 최적 모델로 선택됨  
  - 의료 및 인프라 발달 지역인 **Cluster_0**에서 가장 높은 예측 정확도(**86%**)를 기록  
  - 규모가 작은 **Cluster_3**에서는 **Random Forest**가 XGBoost보다 더 좋은 성능을 보임  

➡️ 결론적으로, **군집 분석을 통해 지역 특성을 반영한 모델링**이 공시지가 예측 성능을 향상시켰으며, 이는 지역 특성이 공시지가 산정에 중요한 역할을 한다는 점을 보여줌.  

---

##  성과 및 학습 포인트
- 실제 도시 공시지가 데이터를 활용하여 다차원자료분석 기법 적용  
- **전처리 → 파생변수 설정 → 변수 선택/차원 축소 → 군집화 → 모델링**까지 전체 분석 파이프라인 경험  
- 데이터 기반 정책 분석과 머신러닝 워크플로우에 대한 실습 경험 축적  

---

# 2. 기상 요인이 가상화폐 종가 변동률 분포에 미치는 영향 분석

##  프로젝트 개요
- **과목**: 베이지안 통계분석 PBL(3학년 2학기, 고려대학교 세종캠퍼스)  
- **기간**: 2023년 10월 – 2023년 12월  
- **목표**: 베이지안 분석을 통해 기상 요인이 가상화폐의 종가 변동률에 영향일 미치는지 분석한다. 

---

##  배경 및 목적
가상화폐 시장은 전통적인 금융시장보다 변동성이 크고 예측이 어렵다.  
기존 연구들은 주로 경제 지표(MFI, Williams %R 등)에 집중했지만,  
기상 요인(기온, 습도, 구름양, 태양 복사량 등)이 **투자자 심리에 미치는 영향**은 충분히 탐구되지 않았다.  

이에 본 프로젝트는 다음을 목표로 하였다:
1. 경제 지표 단독 모델과 경제+기상 데이터 결합 모델을 **베이지안 추정(Bayesian Inference)** 관점에서 비교  
2. **사전분포(Prior), 가능도(Likelihood), 사후분포(Posterior)**를 구성하고  
   **Bayes Factor**를 통해 각 모델의 설명력을 정량적으로 평가  

---

##  데이터 출처
| 구분 | 내용 | 출처 |
|------|------|------|
| 가상화폐 종가 데이터 | Binance API, 전일 대비 종가 변동률 계산 | Binance |
| 경제 지표 | MFI, Williams %R 등 시장 과매수/과매도 지표 | Binance 데이터 기반 |
| 기상 데이터 | 온도, 습도, 구름양, 태양 복사량 | Visual Crossing (뉴욕 지역 기준) |

---

##  방법론

1. **사전분포 설정 (Prior Distribution)**  
   - 종가 변동률의 히스토그램 분석 결과를 기반으로  
     평균 0.06, 표준편차 3.3725의 정규분포를 경험적 사전분포로 설정  

2. **가능도 함수 정의 (Likelihood Function)**  
   - PCA(주성분분석)를 통해 변수 간 상관성을 제거  
   - Gaussian Kernel Density Estimation(GKDE)을 사용하여  
     경제 지표 데이터와 기상 데이터 결합 시의 결합확률분포를 추정  

3. **사후분포 갱신 (Posterior Update)**  
   - 사전분포와 가능도 함수를 결합하여 변동률 구간별 사후 확률 계산  
   - 정규화를 통해 전체 확률 합이 1이 되도록 조정  

4. **신뢰구간 비교 (Confidence Interval)**  
   - 고전적 방법: Bootstrap을 이용한 95% 신뢰구간  
   - 베이지안 방법: HPDI(Highest Posterior Density Interval) 계산  

5. **가설 검정 및 Bayes Factor 계산**  
   - H₀: 변동률이 음수 범위에 속한다  
   - H₁: 변동률이 양수 범위에 속한다  
   - Bayes Factor를 통해 두 모델의 상대적 지지 정도 비교  

---

##  결과 및 분석

| 구분 | 사후분포 평균 | 표준편차 | 95% HPDI | Bayes Factor |
|------|---------------|-----------|------------|---------------|
| 경제 데이터 | 0.03978 | 3.4937 | (-3.0, 3.5) | 1.76 |
| 경제+기상 결합 | 0.02173 | 3.3741 | (-2.5, 2.45) | 1.44 |

- **기상 데이터 결합 시 표준편차 감소** → 불확실성은 약간 줄었음  
- **Bayes Factor 감소(1.76 → 1.44)** → 설명력은 오히려 약화  
- **KL Divergence 분석**에서는 결합 모델이 실제 데이터 분포를 더 잘 근사  

---

##  결론
- 기상 요인의 추가는 모델의 **불확실성을 다소 줄였지만**,  
  사후분포의 설명력은 **크게 향상되지 않았다.**  
- 베이지안 검정 결과, 경제 지표만을 사용한 모델이 대립가설(H₁)에 대해 더 강한 지지를 받음  
- 그러나 KL Divergence 분석에서는 결합 모델이 실제 분포를 더 잘 반영함을 보여줌  
➡️ 즉, **기상 데이터의 영향은 제한적이나, 투자자 심리를 반영하는 보조적 변수로서 가능성을 시사**  

---

##  학습 포인트
- **Bayes Factor, HPDI, Posterior Predictive Check (PPC)** 등  
  베이지안 통계 분석의 핵심 개념을 직접 구현 및 해석  
- 고전적 접근과 베이지안 접근의 결과 차이를 비교하며  
  불확실성·모델 신뢰도 해석 능력을 향상  
- 데이터 과학 프로젝트에서 **비경제적 변수(기상 요인)**를 통합하는 실험적 시도  

---

# 3. EfficientNet을 이용한 결점두 분류 모델 개발

##  프로젝트 개요
- **과목**: T-SUM 데이터 분석 경진대회 
- **기간**: 2024년 9월 – 2024년 12월  
- **목표**: EfficientNet을 기반으로 정상 원두와 결점두(defective beans)를 자동으로 분류하는 모델을 개발하여, 커피 산업의 품질 관리 효율성을 향상시키는 것.  
- **팀원**: 유동남, 김석범, 김수호  

---

##  배경 및 목적
커피의 품질은 원두의 상태에 따라 크게 달라진다.  
현재 대부분의 카페에서는 **결점두 선별 작업이 수작업에 의존**하고 있으며,  
이는 시간과 노동력이 많이 들고 오류 가능성도 높다.  

본 프로젝트는  
1. 결점두 자동 분류를 통한 **품질 관리 자동화**,  
2. **EfficientNet 기반 딥러닝 모델 개발**,  
3. 인간과 모델 간의 성능 비교를 통해 **AI의 실용 가능성 검증**  
을 목표로 수행되었다.

---

##  데이터 전처리 (Preprocessing)
1. **배경 제거 (Background Removal)**  
   - U²-Net 기반 `rembg` 라이브러리를 활용해 원두 외의 배경 제거  

2. **이미지 패딩 (Padding)**  
   - 정사각형 이미지 구성을 위해 긴 변을 기준으로 검정색 패딩 추가  

3. **크기 조정 (Resizing)**  
   - EfficientNet 입력 사이즈인 **224×224**로 통일  

4. **데이터 증강 (Augmentation)**  
   - 수평/수직 반전 및 회전으로 데이터 4배 확장  
   - 정상 원두 500장 + 결점두 500장 → 총 4000장으로 증강  

---

##  모델 구조 및 학습
- **모델**: EfficientNet-B0 (Google AI, 2019)  
- **핵심 특징**:  
  - Depthwise Separable Convolution  
  - Squeeze-and-Excitation Network  
  - Compound Scaling 기법으로 파라미터 효율 극대화  
- **학습 환경**:  
  - Optimizer: Adam  
  - Loss: Binary Cross Entropy  
  - Epochs: 50  
  - Input size: 224×224  
- **평가지표**: Accuracy, Precision, Recall, F1 Score, ROC-AUC  

---

##  결과 및 분석
| 구분 | Precision | Recall | F1-score | Accuracy |
|------|------------|---------|-----------|-----------|
| 모델 (EfficientNet) | **0.9477** | **0.955** | 0.951 | 0.945 |
| 인간 (평균) | 0.899 | 0.899 | 0.899 | 0.935 |

- 모델은 **정밀도(94.8%)**, **재현율(95.5%)**로  
  인간의 평균 분류 정확도(89.9%)를 **초과**함.  
- Confusion Matrix와 ROC Curve에서 모델의 일관된 학습 성능을 확인.  
- 학습 손실(Train vs Test Loss)은 안정적으로 수렴.  

---

##  결론
- EfficientNet 기반 모델은 결점두 분류에서 **인간 수준 이상의 성능**을 달성.  
- 실험 결과, **품질 관리 자동화의 실용 가능성**이 입증되었으며,  
  카페 산업의 인력 부담과 오류율을 줄일 수 있는 잠재력을 보여줌.  

---

##  한계점 및 향후 연구
1. **데이터 다양성 부족** — 한 카페의 데이터만 사용 → 일반화 성능 한계  
2. **정답 데이터 불확실성** — 수작업 라벨링으로 오차 가능성 존재  
3. **향후 방향**  
   - 다중 카페 데이터 확장  
   - EfficientNet v2 / ConvNeXt 등 최신 아키텍처 적용  
   - 전문가 기반 라벨링으로 정답 품질 향상  

---

##  저장소 구조 예시
